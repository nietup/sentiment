{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza sentymentu\n",
    "Implementacja wzorowana na:\n",
    "https://medium.com/@alyafey22/sentiment-classification-from-keras-to-the-browser-7eda0d87cdc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, RepeatVector, Dropout\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import save_model\n",
    "import csv \n",
    "import pandas as pd \n",
    "from IPython.display import display, HTML\n",
    "import h5py\n",
    "from random import shuffle\n",
    "\n",
    "import re\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file = 'dane_treningowe.csv'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        labels = []\n",
    "        text = []\n",
    "\n",
    "        lines = f.readlines()\n",
    "    shuffle(lines)\n",
    "    for line in lines:\n",
    "        data = line.split(',')\n",
    "        if len(data) == 2:\n",
    "            labels.append(data[1])\n",
    "            text.append(data[0].rstrip())\n",
    "    return text,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@brydielonie i miss you girlies lots value:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_text, y_train = load_dataset()\n",
    "x_train_text = x_train_text[:5000]\n",
    "y_train = y_train[:5000]\n",
    "data_text = x_train_text\n",
    "\n",
    "print(x_train_text[3026], \"value: \", y_train[3026])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Eliminujemy:\n",
    "* znaki interpunkcyjne (\".\", \"?\", itp), ponieważ nie niosą ze sobą wartości emocjonalnej\n",
    "\n",
    "Zostawiamy:\n",
    "* \"#\", \"@\" ponieważ słowa używane jako twitter handler/hasztag mogą mieć inne znaczenie niż same słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    out = txt\n",
    "    out = re.sub(r'[.,\"!?:*_-]', '', txt)\n",
    "    out = re.sub('&quot;', '', txt)\n",
    "    out = out.split()\n",
    "    out = [word.lower() for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"a\"', 'a!', 'a.', 'a,', 'a?', 'a:a', 'a*', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(process('\"a\" a! a. a, a? a:a a* &quot;a&quot;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(thresh = 2):\n",
    "    count  = dict()\n",
    "    idx = 1\n",
    "    word_index = dict()\n",
    "    for txt in data_text:\n",
    "        words = process(txt)\n",
    "        for word in words:\n",
    "            if word in count.keys():\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word]  = 1\n",
    "    most_counts = [word for word in count.keys() if count[word]>=thresh]\n",
    "    \n",
    "    sorted_words = sorted(count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    for word in most_counts:\n",
    "        word_index[word] = idx\n",
    "        idx+=1\n",
    "    return word_index, sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMax(data):\n",
    "    max_tokens = 0 \n",
    "    for txt in data:\n",
    "        if max_tokens < len(txt.split()):\n",
    "            max_tokens = len(txt.split())\n",
    "    return max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = getMax(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data):\n",
    "    tokens = []\n",
    "    for txt in data:\n",
    "        words = process(txt)\n",
    "        seq = [0] * max_tokens\n",
    "        i = 0 \n",
    "        for word in words:\n",
    "            start = max_tokens-len(words)\n",
    "            if word.lower() in word_index.keys():\n",
    "                seq[i+start] = word_index[word]\n",
    "            i+=1\n",
    "        tokens.append(seq)        \n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dictionary  3506\n"
     ]
    }
   ],
   "source": [
    "word_index, sorted_words = tokenize()\n",
    "num_words = len(word_index) + 1\n",
    "print('length of the dictionary ', len(word_index))\n",
    "\n",
    "#print(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = create_sequences(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 8\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "\n",
    "model.add(LSTM(units=16, name = \"gru_1\",return_sequences=True))\n",
    "model.add(LSTM(units=8, name = \"gru_32\" ,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=8, name = \"gru_2\" ,return_sequences=True))\n",
    "model.add(LSTM(units=4, name= \"gru_3\"))\n",
    "model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 33, 8)             28056     \n",
      "_________________________________________________________________\n",
      "gru_1 (LSTM)                 (None, 33, 16)            1600      \n",
      "_________________________________________________________________\n",
      "gru_32 (LSTM)                (None, 33, 8)             800       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 33, 8)             0         \n",
      "_________________________________________________________________\n",
      "gru_2 (LSTM)                 (None, 33, 8)             544       \n",
      "_________________________________________________________________\n",
      "gru_3 (LSTM)                 (None, 4)                 208       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 31,213\n",
      "Trainable params: 31,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = [int(i) for i in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4750 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "4750/4750 [==============================] - 16s 3ms/step - loss: 0.6830 - acc: 0.5701 - val_loss: 0.6802 - val_acc: 0.5800\n",
      "Epoch 2/5\n",
      "4750/4750 [==============================] - 11s 2ms/step - loss: 0.6745 - acc: 0.5840 - val_loss: 0.6645 - val_acc: 0.6520\n",
      "Epoch 3/5\n",
      "4750/4750 [==============================] - 12s 2ms/step - loss: 0.6312 - acc: 0.6518 - val_loss: 0.6510 - val_acc: 0.6200\n",
      "Epoch 4/5\n",
      "4750/4750 [==============================] - 10s 2ms/step - loss: 0.5743 - acc: 0.7088 - val_loss: 0.6000 - val_acc: 0.7000\n",
      "Epoch 5/5\n",
      "4750/4750 [==============================] - 11s 2ms/step - loss: 0.5216 - acc: 0.7543 - val_loss: 0.5809 - val_acc: 0.7080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1c857ef6470>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_tokens, y_train_int, validation_split=0.05, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0   59 1243\n",
      "   36  989 3326 2711 1278]\n",
      "\n",
      " prediction for \n",
      " [0.69567317 0.52344674 0.75153387 0.37607855]\n"
     ]
    }
   ],
   "source": [
    "txt = [\"I enjoyed the TV series breaking bad.\",\"Terrible movie\",\"that movie really sucks\",\"I like that movie\"]\n",
    "print(create_sequences(txt)[0])\n",
    "pred = model.predict(create_sequences(txt))\n",
    "print('\\n prediction for \\n',pred[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
